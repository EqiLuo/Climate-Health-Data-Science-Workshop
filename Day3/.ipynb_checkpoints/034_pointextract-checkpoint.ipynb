{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cefcaf9",
   "metadata": {},
   "source": [
    "# Extract Daily Heat Index Values for Point Locations\n",
    "\n",
    "To extract daily heat index values for point locations, we can use the exact same code we used for the national, regional, and district boundaries of Ghana. The only difference is that we are using points, not polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f9225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point \n",
    "import glob\n",
    "# from multiprocessing import Pool\n",
    "# import time\n",
    "from rasterstats import zonal_stats, gen_zonal_stats\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import multiprocessing as mp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc114f05",
   "metadata": {},
   "source": [
    "# Get Daily Heat Index Files\n",
    "\n",
    "First we need to get the location of our daily maximum heat index files and create a list of these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308fef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the data files available using glob  \n",
    "# Construct the file path using os.path.join to ensure compatibility\n",
    "file_path = os.path.join('.', 'data', 'himax-2016', '*.tif')\n",
    "\n",
    "files = sorted(glob.glob(file_path))\n",
    "\n",
    "\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69076cf2",
   "metadata": {},
   "source": [
    "## Load the DHS Clusters\n",
    "\n",
    "Recall the DHS survey data we have used in the previous tutorials, and it is at household level, which means that there is multi-records for a given survey cluster, sharing the same long and lat. <br> For the heat value extractions, since we are looking at the cluster level,  survey data has been aggregated to DHS cluster points. All the numercial columns have been aggregated by mean(), categorical columns were mostly discarded except 'rural_urban' and 'country'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43659f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's load the Ghana DHS survey dataset\n",
    "df = pd.read_csv('./data/DHS_Clusters_Points_Ghana_2014.csv')\n",
    "\n",
    "# Identify records where both longitude and latitude are 0\n",
    "index_to_drop = df[(df['long'] == 0) & (df['lat'] == 0)].index\n",
    "\n",
    "# Drop these records using the drop function\n",
    "df = df.drop(index_to_drop)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca4bbd5-6317-4dbc-9f95-028d859fd188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can check how many DHS Clusters have been sureveyed\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116b9dd-f6ad-4d5b-8694-b6faf9a9108f",
   "metadata": {},
   "source": [
    "Adding Identifiers: This cell adds a new column to the DataFrame, assigning a unique ID to each cluster, starting from 1 up to the number of records. This is useful because later after extracting the heat we may want to join the data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624dd120-b638-4383-acb9-5f4ab228de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create a new column named 'cluster_id' \n",
    "df['cluster_id'] = list(range(1, len(df)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f502d8b-4eb7-4b55-986f-3fae991e0a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f4958",
   "metadata": {},
   "source": [
    "## The DHS data is a CSV file so it needs to be converted into a GeoPandas DataFrame with geometry, not lat/long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929495b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Point\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Create geometry (vector point) from the lon and lat coordinates\n",
    "geometry = [Point(xy) for xy in zip(df['long'], df['lat'])]\n",
    "\n",
    "# Convert pandas dataframe to geopandas dataframe\n",
    "geo_df = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "\n",
    "# Set the CRS to WGS 84 (EPSG:4326)\n",
    "# This step is important as later you may want to reproject the vector files\n",
    "geo_df.set_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "geo_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fed04cc",
   "metadata": {},
   "source": [
    "## Buffer\n",
    "The DHS cluster locations are randomized by up to 10 km. Thus we do not know exactly where a given survey cluster is located. To extract weather and climate data, we need to buffer the cluster by 10 km to increase the accuracy of our extraction.   \n",
    "\n",
    "But our `crs` is in decimal degrees so we need to convert that unit to km. Below is a short function to do just this. Notice that we buffer the points, they get converted to polygons (e.g. circles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59abb66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def km_to_d(km):\n",
    "    \"\"\" Roughly convert Km to degrees: http://wiki.gis.com/wiki/index.php/Decimal_degrees.\n",
    "    \n",
    "    args:\n",
    "        km = km distance\n",
    "    \"\"\"\n",
    "    \n",
    "    d = 0.01 * km / 1.11\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ecb203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer our points\n",
    "geo_df['geometry'] = geo_df['geometry'].buffer(km_to_d(5)) # ~5km buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e65a81e",
   "metadata": {},
   "source": [
    "## Calculating Zonal Statistics\n",
    "From `rasterstats`, we can use the [`zonal_stats`](https://pythonhosted.org/rasterstats/manual.html) function to build our own defined function `zonal` to easily compute the average heat index per geographic area we want. This function will encapsulate all the necessary steps required to perform this computation and can be reused for any raster or vector data input with different statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08571072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonal(rst_in, polys_in, do_stats):\n",
    "    \"\"\"Function will run zonal stats on a raster and a set of polygons. All touched is set to True by default.\n",
    "    \n",
    "    Args:\n",
    "        rst_in (str): File name/path of raster to run zonal stats on.\n",
    "        polys_in (GeoDataFrame): GeoDataFrame of polygons (e.g., administrative boundaries).\n",
    "        do_stats (str): Stats to compute, see rasterstats package for documentation (e.g., 'mean', 'sum').\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A GeoDataFrame with the results of the zonal statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the polygons are in the same CRS as the raster\n",
    "    # Note: It's good practice to read the raster's CRS dynamically rather than hardcoding it\n",
    "    with rasterio.open(rst_in) as src:\n",
    "        raster_crs = src.crs\n",
    "    \n",
    "    polys_in = polys_in.to_crs(raster_crs)\n",
    "    \n",
    "    # Run Zonal Stats\n",
    "    zs_feats = zonal_stats(polys_in, rst_in, stats=do_stats, geojson_out=True, all_touched=True)\n",
    "    \n",
    "    # Turn into GeoDataFrame and set CRS\n",
    "    zgdf = gpd.GeoDataFrame.from_features(zs_feats)\n",
    "    zgdf.crs = polys_in.crs  # Ensuring the resulting GeoDataFrame retains the original CRS\n",
    "    \n",
    "    return zgdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac8b65",
   "metadata": {},
   "source": [
    "Now, let's calculate the daily maximum heat index for **July 1 2016** for each buffered DHS cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5bd017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your vector data \n",
    "polys_in = geo_df[['cluster_id', 'geometry']]\n",
    "polys_in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b4eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the raster file and the statistic you want to compute\n",
    "rst_in = files[182]\n",
    "\n",
    "do_stats = 'mean'  # This could be 'sum', 'mean', 'max', etc.\n",
    "\n",
    "# Run the zonal statistics function\n",
    "dhs = zonal(rst_in, polys_in, do_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it\n",
    "\n",
    "# Initialize empty figure\n",
    "fig = plt.figure(figsize=(10, 15))\n",
    "\n",
    "# Add four axes\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "\n",
    "# DHS plots\n",
    "dhs.plot('mean', legend = True, cmap = 'Reds', ax = ax1)\n",
    "\n",
    "# Plotting the raster data\n",
    "data = rasterio.open(files[182]).read(1)\n",
    "# Mask NoData values (-9999)\n",
    "masked_data = np.ma.masked_where(data == -9999, data)\n",
    "ax2.imshow(masked_data, cmap = 'Reds') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2374dd",
   "metadata": {},
   "source": [
    "## Now Let's Estimate Heat Index Max for Seven Days with a For Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cdad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f454c3bf-8c7b-4329-a508-1f13d7d72fba",
   "metadata": {},
   "source": [
    "# IF YOU ARE USING MAC SYSTEM, CHANGE TO '/himax.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfadb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now write a for loop:\n",
    "# We are iterating through the first seven days of 2016\n",
    "\n",
    "for i, file in enumerate(files[:7]):\n",
    "    \n",
    "    # Logging\n",
    "    print(i, file)\n",
    "    \n",
    "    # get the date string\n",
    "    date = file.split('\\\\himax.')[1].split('-')[0] \n",
    "    print(i, 'my date is', date, '\\n')\n",
    "    \n",
    "    # Run the zonal statistics function\n",
    "    gdf = zonal(file, polys_in, do_stats = 'mean')\n",
    "\n",
    "    # Updates the column name from a generic 'mean' to the specific date\n",
    "    gdf.rename(columns = {'mean' : date}, inplace = True)\n",
    "    \n",
    "    # Make a copy of the first date\n",
    "    if i == 0:\n",
    "        \n",
    "        gdf_out = gdf.copy()\n",
    "    \n",
    "    # Merge days 2-10 onto day 1\n",
    "    else:\n",
    "        gdf_out = pd.merge(gdf_out, gdf[['cluster_id', date]], on = 'cluster_id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a8dbe-2df5-46b7-a46c-3376e87bd77b",
   "metadata": {},
   "source": [
    "If you check the output file `gdf_out`, you can see that the date columns have been added, and they represent the average himax value within the ~5km buffer of the corresponding DHS cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the output file\n",
    "gdf_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1404b12-1ca3-43fb-af93-21bd7515a3b2",
   "metadata": {},
   "source": [
    "# Visualizing time series data\n",
    "The following code plot the heat index values over sseven days for a particular DHS cluster from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0435ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substract the second row - which is Cluster 2 \n",
    "y = gdf_out.iloc[1,2:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c50cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the date columns (starting from the 3rd column) into pandas datetime objects.\n",
    "x = pd.to_datetime(gdf_out.columns.values[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y);\n",
    "plt.plot(x,y)\n",
    "plt.title('Heat Index for DHS Cluster 2 for Jan 1 - 7 2016');\n",
    "plt.xticks(rotation=90)\n",
    "#plt.xlabel('Date')\n",
    "plt.ylabel('Daily Maximum Heat Index Â°C')\n",
    "plt.ylim([0,45])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a11b7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
